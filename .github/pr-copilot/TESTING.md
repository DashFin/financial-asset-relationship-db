# PR Copilot Testing Guide

Comprehensive testing documentation for the PR Copilot automated PR lifecycle management system.

## ğŸ“‹ Overview

The PR Copilot system includes comprehensive test coverage for all Python scripts and integration testing for the complete workflow. Tests are organized into unit tests and integration tests following repository best practices.

## ğŸ§ª Test Structure

```
tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_pr_copilot_generate_status.py    # Status report generation tests
â”‚   â”œâ”€â”€ test_pr_copilot_analyze_pr.py         # PR analysis tests
â”‚   â””â”€â”€ test_pr_copilot_suggest_fixes.py      # Fix suggestion tests
â””â”€â”€ integration/
    â””â”€â”€ test_pr_copilot_workflow.py            # End-to-end workflow tests
```

## ğŸ¯ Test Coverage

### Unit Tests

#### 1. **generate_status.py Tests** (`test_pr_copilot_generate_status.py`)

Tests for PR status report generation:

- âœ… CheckRunInfo dataclass creation
- âœ… PRStatus dataclass creation and immutability
- âœ… Fetching PR status from GitHub API
- âœ… Formatting task checklist (complete/incomplete)
- âœ… Formatting CI check sections
- âœ… Generating complete markdown reports
- âœ… Handling draft PRs
- âœ… Writing output to files and GitHub summaries
- âœ… Edge cases (no checks, unknown mergeable state)

**Coverage:** ~95% of generate_status.py

#### 2. **analyze_pr.py Tests** (`test_pr_copilot_analyze_pr.py`)

Tests for PR complexity analysis:

- âœ… File categorization (Python, JavaScript, tests, workflows, etc.)
- âœ… PR file analysis (empty, small, large changes)
- âœ… Complexity scoring and risk assessment
- âœ… Scope issue detection (long titles, multiple changes, too many files)
- âœ… Related issue parsing from PR body
- âœ… Markdown report generation
- âœ… Configuration loading
- âœ… AnalysisData immutability

**Coverage:** ~90% of analyze_pr.py

#### 3. **suggest_fixes.py Tests** (`test_pr_copilot_suggest_fixes.py`)

Tests for review comment parsing and fix suggestions:

- âœ… Code suggestion extraction (blocks and inline)
- âœ… Comment categorization (critical, bug, question, style, improvement)
- âœ… Actionable comment detection
- âœ… Review comment parsing
- âœ… Priority sorting
- âœ… Fix proposal generation
- âœ… Long body truncation
- âœ… Configuration loading with defaults

**Coverage:** ~90% of suggest_fixes.py

### Integration Tests

#### **Workflow Integration** (`test_pr_copilot_workflow.py`)

End-to-end tests for the complete PR Copilot system:

- âœ… Configuration file existence and validity
- âœ… Workflow file existence and validity
- âœ… Script file existence
- âœ… Requirements file validity
- âœ… Complete status generation workflow
- âœ… Complete PR analysis workflow
- âœ… Complete fix suggestion workflow
- âœ… Workflow trigger configuration
- âœ… Workflow job configuration
- âœ… Workflow permissions
- âœ… Agent settings validation
- âœ… Trigger settings validation
- âœ… Scope settings validation
- âœ… Auto-merge settings validation
- âœ… Documentation existence and content

**Coverage:** Complete workflow validation

## ğŸš€ Running Tests
### Quick Start with Test Runner Script

The easiest way to run all PR Copilot tests is using the provided test runner script, which automatically sets up a virtual environment and installs dependencies:

```bash
# Run all tests with automatic virtual environment setup
.github/pr-copilot/scripts/run_tests.sh

# Run with coverage report
.github/pr-copilot/scripts/run_tests.sh --coverage
```


### Automated Test Script (Recommended)

Use the provided test script that automatically sets up a virtual environment:

```bash
# Run all tests with automatic virtual environment setup
.github/pr-copilot/scripts/run_tests.sh

# Run with coverage report
.github/pr-copilot/scripts/run_tests.sh --coverage
```

**Benefits:**
- âœ… Automatic virtual environment creation and management
- âœ… Isolated dependency installation
- âœ… Consistent test environment across runs
- âœ… Automatic cleanup on exit
**Benefits of using the test runner:**
- âœ… Automatic virtual environment creation and management
- âœ… Isolated dependency installation
- âœ… Consistent test execution across environments
- âœ… Automatic cleanup on exit
- âœ… Color-coded output for better readability

The script creates a virtual environment at `.venv-pr-copilot/` in the repository root, which is automatically excluded from version control.

### Manual Test Execution

### Run All PR Copilot Tests

```bash
# Run all PR Copilot unit tests
pytest tests/unit/test_pr_copilot_*.py -v

# Run integration tests
pytest tests/integration/test_pr_copilot_workflow.py -v

# Run all PR Copilot tests
pytest tests/unit/test_pr_copilot_*.py tests/integration/test_pr_copilot_workflow.py -v
```

### Run Specific Test Files

```bash
# Test status generation
pytest tests/unit/test_pr_copilot_generate_status.py -v

# Test PR analysis
pytest tests/unit/test_pr_copilot_analyze_pr.py -v

# Test fix suggestions
pytest tests/unit/test_pr_copilot_suggest_fixes.py -v

# Test workflow integration
pytest tests/integration/test_pr_copilot_workflow.py -v
```

### Run Specific Test Functions

```bash
# Test specific functionality
pytest tests/unit/test_pr_copilot_generate_status.py::test_fetch_pr_status -v
pytest tests/unit/test_pr_copilot_analyze_pr.py::test_assess_complexity_high -v
pytest tests/unit/test_pr_copilot_suggest_fixes.py::test_categorize_comment_critical -v
```

### Run with Coverage

```bash
# Generate coverage report
pytest tests/unit/test_pr_copilot_*.py --cov=.github/pr-copilot/scripts --cov-report=html

# View coverage in browser
open htmlcov/index.html
```

## ğŸ“Š Test Scenarios

### Scenario 1: Complete PR Status Report

**Test:** `test_generate_status_integration`

Validates:
- PR metadata extraction
- Review status aggregation
- CI check status
- Merge readiness assessment
- Markdown formatting

### Scenario 2: High-Risk PR Analysis

**Test:** `test_assess_complexity_high`

Validates:
- Complexity scoring for large PRs
- Risk level assessment
- Large file detection
- Scope issue identification

### Scenario 3: Critical Review Feedback

**Test:** `test_categorize_comment_critical`

Validates:
- Security issue detection
- Priority assignment
- Actionable item extraction
- Fix proposal generation

### Scenario 4: Workflow Configuration

**Test:** `test_workflow_triggers_configuration`

Validates:
- All required triggers present
- Correct event types
- Proper job dependencies
- Permission settings

## ğŸ” Test Data

### Mock PR Data

Tests use realistic mock data:

```python
mock_pr = {
    "number": 42,
    "title": "Add new feature for user authentication",
    "author": "contributor",
    "commits": 8,
    "files_changed": 12,
    "additions": 250,
    "deletions": 75,
    "labels": ["enhancement", "security"],
    "mergeable": True,
    "reviews": [{"state": "APPROVED"}],
    "checks": [{"name": "CI", "conclusion": "success"}]
}
```

### Mock Review Comments

```python
mock_comment = {
    "author": "reviewer",
    "body": "Please fix this security vulnerability",
    "category": "critical",
    "priority": 1,
    "file": "auth.py",
    "line": 42
}
```

## ğŸ› ï¸ Test Utilities

### Fixtures

Common fixtures used across tests:

- `mock_github_client`: Mock GitHub API client
- `mock_pr`: Complete PR object with all data
- `mock_reviews`: Review objects with various states
- `mock_check_runs`: CI check run objects
- `mock_env_vars`: Environment variables for scripts

### Mocking Strategy

Tests use `unittest.mock` for:
- GitHub API calls
- File system operations
- Environment variables
- External dependencies

## âœ… Test Checklist

Before deploying PR Copilot changes:

- [ ] All unit tests pass
- [ ] All integration tests pass
- [ ] Coverage > 85% for all scripts
- [ ] No test warnings or errors
- [ ] Configuration files validated
- [ ] Workflow file validated
- [ ] Documentation updated
- [ ] Edge cases covered

## ğŸ› Debugging Tests

### Enable Verbose Output

```bash
pytest tests/unit/test_pr_copilot_*.py -vv
```

### Show Print Statements

```bash
pytest tests/unit/test_pr_copilot_*.py -s
```

### Run Failed Tests Only

```bash
pytest tests/unit/test_pr_copilot_*.py --lf
```

### Debug with PDB

```bash
pytest tests/unit/test_pr_copilot_*.py --pdb
```

## ğŸ“ˆ Continuous Integration

Tests run automatically in CI/CD:

```yaml
# .github/workflows/ci.yml
- name: Run PR Copilot Tests
  run: |
    pytest tests/unit/test_pr_copilot_*.py -v
    pytest tests/integration/test_pr_copilot_workflow.py -v
```

## ğŸ”„ Test Maintenance

### Adding New Tests

When adding new functionality:

1. Create test file in appropriate directory
2. Follow naming convention: `test_pr_copilot_<module>.py`
3. Include docstrings for all test functions
4. Use fixtures for common setup
5. Test both success and failure cases
6. Update this documentation

### Updating Existing Tests

When modifying functionality:

1. Update affected test cases
2. Ensure backward compatibility
3. Add tests for new edge cases
4. Verify coverage remains high
5. Update test documentation

## ğŸ“š Best Practices

### Test Organization

- **One test per function**: Each test validates one specific behavior
- **Clear naming**: Test names describe what they validate
- **Arrange-Act-Assert**: Follow AAA pattern
- **Minimal mocking**: Mock only external dependencies
- **Realistic data**: Use data similar to production

### Test Quality

- **Fast execution**: Tests should run quickly
- **Isolated**: Tests don't depend on each other
- **Deterministic**: Same input always produces same output
- **Comprehensive**: Cover happy path and edge cases
- **Maintainable**: Easy to understand and update

## ğŸ”— Related Documentation

- [PR Copilot README](README.md) - User guide and features
- [Setup Guide](SETUP.md) - Installation and configuration
- [Configuration Reference](../.github/pr-copilot-config.yml) - All settings
- [Workflow File](../.github/workflows/pr-copilot.yml) - GitHub Actions workflow

## ğŸ’¡ Tips

- Run tests before committing changes
- Use coverage reports to find untested code
- Add tests for bug fixes to prevent regression
- Keep tests simple and focused
- Update tests when requirements change

## ğŸ¤ Contributing

To contribute test improvements:

1. Write tests for new features
2. Ensure all tests pass
3. Maintain or improve coverage
4. Follow existing patterns
5. Document test scenarios
6. Submit PR with test changes

## ğŸ“ Support

For test-related issues:

- Check test output for specific errors
- Review mock data and fixtures
- Verify environment setup
- Consult existing test examples
- Open issue with test failure details

---

**Questions about testing?** Open an issue or refer to the main [TESTING.md](../../TESTING.md) guide!
